version: "0.1-beta"

listeners:
  egress_traffic:
    address: 0.0.0.0
    port: 12000
    message_format: openai
    timeout: 30s

llm_providers:
  - name: qwen3:0.6b
    provider_interface: openai
    # This configuration is converted to Envoy and run inside Docker.
    endpoint: host.docker.internal:11434
    model: qwen3:0.6b
    default: true

tracing:
  random_sampling: 100
  trace_arch_internal: true

version: v0.1.0

listeners:
  egress_traffic:
    address: 0.0.0.0
    port: 12000
    message_format: openai
    timeout: 30s

llm_providers:
  # Use ollama directly, since we can't inherit OPENAI_BASE_URL etc and need
  # to hard-code the model anyway.
  - model: ollama/qwen3:0.6b
    # This configuration is converted to Envoy and run inside Docker.
    base_url: http://host.docker.internal:11434
    default: true

tracing:
  random_sampling: 100
  trace_arch_internal: true

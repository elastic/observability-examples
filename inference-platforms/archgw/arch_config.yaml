version: v0.1.0

listeners:
  egress_traffic:
    address: 0.0.0.0
    port: 12000
    message_format: openai
    timeout: 30s

llm_providers:
  - model: ollama/qwen3:0.6b
    provider_interface: openai
    # This configuration is converted to Envoy and run inside Docker.
    base_url: http://host.docker.internal:11434
    default: true

tracing:
  random_sampling: 100
  trace_arch_internal: true

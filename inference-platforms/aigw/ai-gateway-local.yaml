# Copyright Envoy AI Gateway Authors
# SPDX-License-Identifier: Apache-2.0
# The full text of the Apache license is available in the LICENSE file at
# the root of the repo.

# Configuration for running Envoy AI Gateway with an OpenAI compatible
# inference platform, defaulting to Ollama.
#
# Override with the OPENAI_HOST and OPENAI_PORT environment variables.
# For example, to use RamaLama you would set OPENAI_PORT=8080.
#
# TODO: Remove after https://github.com/envoyproxy/ai-gateway/issues/1211
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: aigw-run
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: aigw-run
  namespace: default
spec:
  gatewayClassName: aigw-run
  listeners:
    - name: http
      protocol: HTTP
      port: 1975
  infrastructure:
    parametersRef:
      group: gateway.envoyproxy.io
      kind: EnvoyProxy
      name: envoy-ai-gateway
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: EnvoyProxy
metadata:
  name: envoy-ai-gateway
  namespace: default
spec:
  logging:
    level:
      default: error
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIGatewayRoute
metadata:
  name: aigw-run
  namespace: default
spec:
  parentRefs:
    - name: aigw-run
      kind: Gateway
      group: gateway.networking.k8s.io
  # Simple rule: route everything to Ollama
  rules:
    - matches:
        - headers:
            - type: RegularExpression
              name: x-ai-eg-model
              value: .*
      backendRefs:
        - name: ollama
          namespace: default
      timeouts:
        request: 120s
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: ollama
  namespace: default
spec:
  endpoints:
    # Use fqdn, not ip, to allow changing to host.docker.internal
    - fqdn:
        hostname: ${OPENAI_HOST:=127.0.0.1.nip.io}  # Resolves to 127.0.0.1
        port: ${OPENAI_PORT:=11434}  # Default to Ollama's port
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: ollama
  namespace: default
spec:
  timeouts:
    request: 3m
  schema:
    name: OpenAI
  backendRef:
    name: ollama
    kind: Backend
    group: gateway.envoyproxy.io
    namespace: default
---
# By default, Envoy Gateway sets the buffer limit to 32kiB which is not
# sufficient for AI workloads. This ClientTrafficPolicy sets the buffer limit
# to 50MiB as an example.
# TODO: Remove ofter https://github.com/envoyproxy/ai-gateway/issues/1212
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: ClientTrafficPolicy
metadata:
  name: client-buffer-limit
  namespace: default
spec:
  targetRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: aigw-run
  connection:
    bufferLimit: 50Mi

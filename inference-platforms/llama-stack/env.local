# OpenAI-compatible endpoint configuration
OPENAI_BASE_URL=http://localhost:8321/v1
OPENAI_API_KEY=unused
# Models require `provider_id/` prefix, in this case `openai`
CHAT_MODEL=openai/qwen3:0.6b
AGENT_MODEL=openai/qwen3:1.7b

# OpenTelemetry configuration
OTEL_SERVICE_NAME=llama-stack
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

# Disable resource detectors by default
OTEL_PYTHON_DISABLED_RESOURCE_DETECTORS=all

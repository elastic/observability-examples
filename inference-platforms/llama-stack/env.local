# Override default ENV variables for llama-stack
OPENAI_BASE_URL=http://localhost:8321/v1/openai/v1
OPENAI_API_KEY=unused
CHAT_MODEL=llama3.2:1b

# Variable name used by llama-stack
INFERENCE_MODEL=llama3.2:1b

# OpenTelemetry configuration
TELEMETRY_SINKS=otel_trace,otel_metric
# Note: there's not yet ENV variable support for endpoints
# See https://github.com/meta-llama/llama-stack/issues/783

OTEL_SERVICE_NAME=llama-stack
# Disable resource detectors by default
OTEL_PYTHON_DISABLED_RESOURCE_DETECTORS=all

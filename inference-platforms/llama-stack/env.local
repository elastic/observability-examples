# Override default ENV variables for llama-stack
OPENAI_BASE_URL=http://localhost:8321/v1/openai/v1
OPENAI_API_KEY=unused
CHAT_MODEL=llama3.2:1b

# Variables used by llama-stack
OLLAMA_URL=http://localhost:11434
INFERENCE_MODEL=llama3.2:1b

# OpenTelemetry configuration
TELEMETRY_SINKS=otel_trace,otel_metric
OTEL_SERVICE_NAME=llama-stack
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

# Disable resource detectors by default
OTEL_PYTHON_DISABLED_RESOURCE_DETECTORS=all

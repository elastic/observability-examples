# Update this with your real OpenAI API key
OPENAI_API_KEY=

# Uncomment to use Ollama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=unused
# CHAT_MODEL=qwen2.5:3b

# Uncomment and complete if you want to use Azure OpenAI Service
## "Azure OpenAI Endpoint" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com/
## "API key 1 (or 2)" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_API_KEY=
## "Inference version" from https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
# OPENAI_API_VERSION=2024-10-01-preview
## "Name" from https://oai.azure.com/resource/deployments
# CHAT_MODEL=YOUR_DEPLOYMENT_NAME

# OpenTelemetry configuration for SemanticKernel
SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS=true
SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS_SENSITIVE=true
OTEL_DOTNET_AUTO_TRACES_ADDITIONAL_SOURCES="Microsoft.SemanticKernel*"

# OTEL_EXPORTER_* variables are not required. If you would like to change your
# OTLP endpoint to Elastic APM server using HTTP, uncomment the following:
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:8200
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

OTEL_SERVICE_NAME=genai-function-calling

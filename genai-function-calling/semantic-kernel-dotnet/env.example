# Update this with your real OpenAI API key
OPENAI_API_KEY=

# Uncomment to use Ollama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=unused
# # This works when you supply a major_version parameter in your prompt. If you
# # leave it out, you need to update this to qwen2.5:3b to proceed the tool call.
# CHAT_MODEL=qwen2.5:0.5b

# Uncomment to use RamaLama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:8080/v1
# OPENAI_API_KEY=unused
# # This works when you supply a major_version parameter in your prompt. If you
# # leave it out, you need to update this to qwen2.5:3b to proceed the tool call.
# CHAT_MODEL=qwen2.5:0.5b

# Uncomment and complete if you want to use Azure OpenAI Service
## "Azure OpenAI Endpoint" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com/
## "API key 1 (or 2)" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_API_KEY=
## "Inference version" from https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
# OPENAI_API_VERSION=2024-10-01-preview
## "Name" from https://oai.azure.com/resource/deployments
# CHAT_MODEL=YOUR_DEPLOYMENT_NAME

# OpenTelemetry configuration for SemanticKernel
SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS=true
SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS_SENSITIVE=true
OTEL_DOTNET_AUTO_TRACES_ADDITIONAL_SOURCES="ElasticsearchVersionAgent,Microsoft.SemanticKernel*,Experimental.ModelContextProtocol*"

# Default to send logs, traces and metrics to an OpenTelemetry collector,
# accessible via localhost. For example, Elastic Distribution of OpenTelemetry
# (EDOT) Collector.
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

OTEL_SERVICE_NAME=genai-function-calling

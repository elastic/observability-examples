# Update this with your real OpenAI API key
OPENAI_API_KEY=

# Uncomment to use Ollama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=unused
# # This works when you supply a major_version parameter in your prompt. If you
# # leave it out, you need to update this to qwen2.5:3b to proceed the tool call.
# CHAT_MODEL=qwen2.5:0.5b

# Uncomment to use RamaLama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:8080/v1
# OPENAI_API_KEY=unused
# # This works when you supply a major_version parameter in your prompt. If you
# # leave it out, you need to update this to qwen2.5:3b to proceed the tool call.
# CHAT_MODEL=qwen2.5:0.5b

# Uncomment and complete if you want to use Azure OpenAI Service
## "Azure OpenAI Endpoint" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com/
## "API key 1 (or 2)" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_API_KEY=
## "Inference version" from https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
# OPENAI_API_VERSION=2024-10-01-preview
## "Name" from https://oai.azure.com/resource/deployments
# CHAT_MODEL=YOUR_DEPLOYMENT_NAME

OTEL_SERVICE_NAME=genai-function-calling

# Default to send logs, traces and metrics to an OpenTelemetry collector,
# accessible via localhost. For example, Elastic Distribution of OpenTelemetry
# (EDOT) Collector.
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

# Change to 'false' to hide prompt and completion content
OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true

# Export metrics every 3 seconds instead of every minute
OTEL_METRIC_EXPORT_INTERVAL=3000
# Export traces every 3 seconds instead of every 5 seconds
OTEL_BSP_SCHEDULE_DELAY=3000
# Change to affect behavior of which resources are detected. Note: these
# choices are specific to the language, in this case Python.
OTEL_EXPERIMENTAL_RESOURCE_DETECTORS=process_runtime,os,otel,telemetry_distro

# Update this with your real OpenAI API key
OPENAI_API_KEY=

# Uncomment to use Ollama instead of OpenAI.
# Note: OpenAI's OPENAI_BASE_URL var ends in /v1, while this doesn't, until:
# https://github.com/spring-projects/spring-ai/issues/2415
# OPENAI_BASE_URL=http://localhost:11434
# OPENAI_API_KEY=unused
# CHAT_MODEL=qwen2.5:3b

# Uncomment and complete if you want to use Azure OpenAI Service
## "Azure OpenAI Endpoint" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com/
## "API key 1 (or 2)" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_API_KEY=
## "Inference version" from https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
# OPENAI_API_VERSION=2024-10-01-preview
## "Name" from https://oai.azure.com/resource/deployments
# CHAT_MODEL=YOUR_DEPLOYMENT_NAME

# OpenTelemetry configuration for Spring AI
# Disable spring-boot built-in HTTP instrumentation because it does not follow OTel conventions and ends up as a sibling
# of javaagent's instrumentation. Also exclude Azure OpenAI which cannot be active at the same time as OpenAI until
# https://github.com/spring-projects/spring-ai/issues/2392
SPRING_AUTOCONFIGURE_EXCLUDE=org.springframework.boot.actuate.autoconfigure.observation.web.client.HttpClientObservationsAutoConfiguration,org.springframework.ai.autoconfigure.azure.openai.AzureOpenAiAutoConfiguration
# Disable OpenAI which cannot be active at the same time as Azure OpenAI until
# https://github.com/spring-projects/spring-ai/issues/2392
# SPRING_AUTOCONFIGURE_EXCLUDE=org.springframework.ai.autoconfigure.openai.OpenAiAutoConfiguration
OTEL_INSTRUMENTATION_MICROMETER_ENABLED=true

# OTEL_EXPORTER_* variables are not required. If you would like to change your
# OTLP endpoint to Elastic APM server using HTTP, uncomment the following:
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:8200
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

OTEL_SERVICE_NAME=genai-function-calling

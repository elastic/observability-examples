# Update this with your real OpenAI API key
OPENAI_API_KEY=

# Uncomment to use Ollama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=unused
# # This works when you supply a major_version parameter in your prompt. If you
# # leave it out, you need to update this to qwen3:1.7b to proceed the tool call.
# CHAT_MODEL=qwen3:0.6b

# Uncomment to use RamaLama instead of OpenAI
# OPENAI_BASE_URL=http://localhost:8080/v1
# OPENAI_API_KEY=unused
# # This works when you supply a major_version parameter in your prompt. If you
# # leave it out, you need to update this to qwen3:1.7b to proceed the tool call.
# CHAT_MODEL=qwen3:0.6b

# Uncomment and complete if you want to use Azure OpenAI Service
## "Azure OpenAI Endpoint" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com/
## "API key 1 (or 2)" in https://oai.azure.com/resource/overview
# AZURE_OPENAI_API_KEY=
## "Inference version" from https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation
# OPENAI_API_VERSION=2024-10-01-preview
## "Name" from https://oai.azure.com/resource/deployments
# CHAT_MODEL=YOUR_DEPLOYMENT_NAME

# OpenTelemetry configuration for Spring AI
# Disable spring-boot built-in HTTP instrumentation because it does not follow OTel conventions and ends up as a sibling
# of javaagent's instrumentation. Also exclude Azure OpenAI which cannot be active at the same time as OpenAI until
# https://github.com/spring-projects/spring-ai/issues/2392
SPRING_AUTOCONFIGURE_EXCLUDE=org.springframework.boot.actuate.autoconfigure.observation.web.client.HttpClientObservationsAutoConfiguration,org.springframework.ai.autoconfigure.azure.openai.AzureOpenAiAutoConfiguration
# Disable OpenAI which cannot be active at the same time as Azure OpenAI until
# https://github.com/spring-projects/spring-ai/issues/2392
# SPRING_AUTOCONFIGURE_EXCLUDE=org.springframework.ai.autoconfigure.openai.OpenAiAutoConfiguration
OTEL_INSTRUMENTATION_MICROMETER_ENABLED=true

# Default to send logs, traces and metrics to an OpenTelemetry collector,
# accessible via localhost. For example, Elastic Distribution of OpenTelemetry
# (EDOT) Collector.
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

OTEL_SERVICE_NAME=genai-function-calling
